# Review and Recommendations for Dexter Repository Architecture

## Current Architecture Overview

The **Dexter** project is currently structured as a single repository containing multiple distinct components: a web UI, an API/backend, and an AI “MCP” (Model Context Protocol) server. All of these live under one codebase (a monorepo style). For example, the token-ai/ directory holds the core Node.js code (the analyzer logic, web server, and MCP agent) while the public/ folder contains static frontend assets (HTML/JS for the live agent UI and dashboard)[\[1\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/AGENTS.md#L5-L11). In production, two Node processes run: one serving the UI/API (port 3017) and one for the MCP HTTP endpoint (port 3930)[\[2\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L30-L38)[\[3\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L90-L98). The UI process (token-ai/server.js) serves static pages and provides routes for things like authentication, wallet management, and real-time events[\[4\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/server.js#L64-L73)[\[5\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/server.js#L14-L21). The MCP process (token-ai/mcp/http-server-oauth.mjs) handles the AI agent’s streaming conversation logic and tool usage, exposed at /mcp (with OAuth protection for external connectors like Claude or ChatGPT)[\[6\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L1-L9).

**Supabase** is partially integrated: the front-end supports Supabase OAuth (magic-link login) for user accounts[\[7\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L84-L88), and the backend can use a Postgres database (with Prisma) for persistence (intended to point to Supabase’s Postgres)[\[8\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L122-L130). In practice, a local Postgres was used during development, but the plan is to migrate fully to Supabase for easier deployment[\[9\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L123-L131).

Overall, the project’s goal is clear: **Users log in, then interact with AI agents (via text or voice) using their account**, with the AI able to perform various **“tool” actions** (web searches, wallet operations, trading commands, etc.) on the user’s behalf. The architecture supports this via the MCP server: the AI model (GPT-4 or Claude) is orchestrated through the MCP, which streams responses and can invoke tool functions on the backend[\[10\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L75-L83)[\[11\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L91-L100). The front-end (browser) connects to the MCP either directly (via a proxy endpoint /mcp-proxy) or through OAuth tokens, so that each user’s requests are authorized and their tool actions (like trading) are tied to their account[\[12\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L97-L105).

## Issues with the Current Monolithic Setup

It’s evident that the current codebase has become **“messy” and overbogged with technical debt**, as you described. There are several signs of this:

- **Large, entangled server code with deprecated sections:** The main server file grew to ~1700 lines and contains many blocks of code that have been disabled (if(false) around old routes) after partial refactors[\[13\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L3-L11). For instance, an internal analysis shows **three different systems’ code coexisting** in server.js: an **old voice debug system**, a **failed OAuth implementation**, and an **original wallet management API**, all superseded by newer implementations but still present (just commented or disabled)[\[14\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L54-L63). This duplication makes it hard to know which code is active and which is legacy. In fact, the public/agent-live.html front-end might still be trying to call some of those now-disabled endpoints[\[15\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L73-L81), leading to confusion or broken functionality. Keeping dead code around significantly increases cognitive load and risk of inconsistencies.
- **Mixed concerns in one repo:** The repository contains front-end static files, backend web server, AI agent logic, and even auxiliary tooling (like the “Inspector” TypeScript subproject) all together. While a monorepo can work, in this case the concerns are tightly coupled. There’s no clear separation between the **“website frontend”**, the **user-facing API/backend**, and the **MCP agent service** – they are all versioned and deployed as one unit. This has made the project feel **overly complex and unwieldy**, since every small change or deployment touches the whole system. For example, the OAuth login pages and static HTML are served by the same Node app that also manages spawning AI analysis runs and interfacing with OpenAI – a lot of responsibility in one process. Ideally, each of these major pieces would be isolated (at least logically, if not in separate repos) to reduce interference and complexity.
- **Partially implemented transitions:** As noted, some features were rewritten in a new module but not fully cleaned up. The **voice debug logging** is one example – there are two different implementations (one in server.js using a voiceLog ring buffer, and a newer one in server/routes/realtime.js using realtimeDebugSessions), with different data structures and auth checks[\[16\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L8-L17)[\[17\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L19-L28). This redundancy indicates a refactor that didn’t complete, and it risks the UI or agent using the wrong one. Similarly, OAuth routes were first attempted directly in server.js then replaced by a dedicated mcpProxy.js router (with the old code left disabled)[\[18\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L34-L42). This pattern suggests the codebase has **accumulated cruft** that makes it hard to maintain or even understand which codepath is active.
- **Inconsistent architecture (single repo vs multiple services):** You mentioned feeling that the **MCP server should have been its own repo** from the start – and indeed, conceptually it’s a separate service (it even runs as its own systemd unit)[\[3\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L90-L98). Keeping it inside the same project as the UI means the boundary between the two is not as clean as it could be. For example, both the UI and MCP share environment config and some utility code. The build process requires copying the .env to the token-ai/.env subdirectory for the MCP[\[19\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L76-L84), and both services are started via one combined npm project. This tight coupling can make deployments or testing more cumbersome (you can’t easily version or deploy the MCP independently of the front-end, etc.). It also likely makes the repository feel “bogged down” because it’s trying to serve two masters (web app vs. AI engine).
- **Lack of clarity in tool architecture (local vs remote):** The design for tools in the MCP is powerful but complex – the MCP can provide various toolsets (wallet, web, voice, trading, etc.) to the AI[\[20\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L14-L22)[\[21\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L102-L110). However, there seems to be confusion about **local tools vs. AI-native capabilities**. You indicated that originally the ChatGPT/Claude connectors were supposed to handle _all_ tooling, possibly by leveraging those models’ own tool-use abilities, and that having local fallback tools might be unnecessary. In the current implementation, the MCP does host a suite of local tool functions (see token-ai/mcp/tools/\*.mjs for trading, web search, etc.), and the AI model calls them via function-calling. This gives you full control, but it also means a lot of custom code to maintain (and duplicate logic that an OpenAI plugin or built-in browsing might handle). The **ChatGPT connector note** in your docs confirms that ChatGPT as a client only expects certain tools (“search” and “fetch”), which you provide in the format it expects[\[11\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L91-L100). This indicates you’ve tailored the MCP to work with ChatGPT’s limited tool interface, essentially bending your system to the AI’s paradigm. While that’s clever, it adds another layer of complexity (mapping your internal tools to external ones). The current setup therefore has an _overlap of responsibilities_ – some tool logic lives locally, but you also rely on the AI (especially in voice mode) to decide how to use them. If not designed carefully, this can lead to the very reliability issues you’ve seen (tools not being called or returning results properly).
- **Reliability issues in the voice agent:** The **real-time voice agent** feature is clearly one of the most innovative parts of Dexter, but it’s also where a lot of pain is coming from. As you noted, the current voice agent “doesn’t even use the SDK” and was done manually, leading to frequent tool-call errors ~90% of the time. This is borne out by the project notes: for example, logs showed constant errors like **“invalid_tool_call_id… not found in conversation”** when the voice agent attempted to return function results[\[22\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L5-L13). Essentially, the client code was using the wrong IDs when sending function outputs back to the model, so the OpenAI API rejected them. You had to patch this by capturing the correct call_id from the model event and using that[\[23\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L23-L31)[\[24\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L33-L37). Similarly, issues like tools not being registered or function schemas being too strict caused “unknown_tool” errors and required fixes[\[25\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L47-L56)[\[26\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L51-L58). The fact that multiple low-level bugs had to be fixed (ID handling, schema mismatches, tool list sync) shows how **fragile the DIY approach to the voice agent** has been. OpenAI’s own documentation strongly encourages using their **TypeScript Agents SDK** for building real-time voice agents, precisely to avoid these pitfalls – _“If you are new to building voice agents, we recommend using the Realtime Agents in the TypeScript Agents SDK to get started”_[\[27\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L8-L15). Since Dexter’s voice implementation was custom, it meant reimplementing a lot of this logic, which introduced complexity and bugs. This is likely why things “feel broken” – the voice agent works only in parts because it’s complicated to maintain all the moving pieces (audio streaming, VAD/turn-taking, function call coordination, etc.) without the support of a robust SDK.
- **Partial migration to new tech (Supabase, etc.):** Another source of complexity is the in-progress migration to Supabase. Right now, the system can run without a DB (writing analysis reports to files) or with a local Prisma/Postgres, or pointed at Supabase’s Postgres. In other words, there are conditional code paths depending on environment setup (demo mode vs. full DB mode). Until this migration is completed, there’s extra weight in the code to handle both scenarios. For example, the code that spawns analyzer processes writes to token-ai/reports/\* when no DB is present[\[28\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L124-L130). Likewise, auth flows have to consider a “demo mode” without an external identity provider vs. the real OAuth with Supabase or another OIDC. This **flexibility is useful for development,** but it also adds more surface area for bugs and configuration drift. Since your goal now is to go all-in on Supabase (for both Auth and DB), the code will simplify once you drop the legacy/local modes. Until then, though, the presence of both paths contributes to the “messy” feeling.

In summary, the current repo is a **monolith that tries to do everything** – serve web content, handle logins, orchestrate AI sessions, execute tool functions, and even run background analyses. It’s no surprise that it feels overwhelming. There’s evidence of **half-finished refactors and overlapping systems** which make the code hard to reason about and error-prone[\[29\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L71-L79)[\[30\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L98-L105). These issues are driving the understandable urge to “burn it down” and rebuild from a clean slate.

## Rebuild vs. Refactor: What to Keep and What to Change

You’re at a crossroads deciding whether to **scrap this version and rebuild from the ground up** or to incrementally refactor the existing code. Let’s consider both approaches in light of preserving “the spirit of the functionality” (i.e. all the features and goals) while making the system more manageable.

**1\. Full Rebuild (New Version from Foundations):** This path means designing a fresh architecture, likely splitting into separate projects for each major component. The benefits would be a clean separation of concerns and an opportunity to use modern frameworks/SDKs from the start. For example, you could create:

- _Front-End Web App:_ A dedicated repo for the user-facing site (could be a React/Next.js app, or even just cleaner static pages). This would handle the UI for chat/voice interaction and utilize Supabase JS SDK for auth and any direct database calls. By isolating the front-end, you can focus on user experience without the tangle of backend logic in the same codebase.
- _Backend API:_ A separate service for user management, account linking, and serving any HTTP APIs the front-end needs (other than the AI stream). This could be a lightweight Node.js (or Deno, etc.) service that exposes endpoints for things like profile info, linking accounts (as hinted by link.html and linking.js), and perhaps serves the static files if you stick with simple HTML/JS. With Supabase handling authentication, this service mainly ensures users have the data they need (e.g. retrieving their chat history or available wallets from the database).
- _MCP Agent Server:_ A standalone service (its own repo) that implements the Model Context Protocol and tool plugins. This would essentially be the “brain” service. It could be rebuilt with a clearer structure – for instance, using TypeScript and the official OpenAI Agents SDK for the voice/chat agent logic, and a well-defined plugin interface for tools. The MCP server would expose a streaming API (as it does now at /mcp) and could be consumed by both your own front-end and external AI clients (ChatGPT plugin or Claude) in a standardized way. By isolating this, you can iterate on the AI agent independently, and even scale it separately if needed.

Taking the full rebuild approach would let you **eliminate the accumulated cruft** and only carry over the “good parts” by re-implementing them cleanly. It sounds drastic, but given the level of frustration with the current state, it might actually be faster than untangling everything. You already have well-defined goals and even documentation of what each part should do – you can treat the existing project as a prototype/blueprint. For instance, you know what tools the AI needs (wallet listing, trading execution, etc.) and how they should work. You can re-code those in the new MCP service with proper schema definitions from the start (ensuring optional params, correct IDs, etc., which you learned the hard way before).

The downsides of a rebuild are the **time and effort** – you’ll essentially recode a lot of logic. However, considering how much time has been spent fighting fires (e.g. debugging voice agent issues, toggling environment hacks), a fresh start could actually save time long-term. Also, you don’t have to throw everything away: you can copy over chunks of code that are known to work (e.g. your Prisma schema for the DB, the Supabase auth integration bits, or certain utility functions). But you would **avoid copying the broken or obsolete parts** and instead implement them correctly with benefit of hindsight.

**2\. Incremental Refactor:** This approach means **gradually cleaning up and modularizing the existing repository** rather than abandoning it. It might involve steps like: removing all the disabled/dead code, splitting large files into smaller modules, and perhaps reorganizing the directory structure to clearly delineate front-end vs backend vs MCP. You could also introduce a package-based monorepo structure (e.g. using Yarn workspaces or PNPM) where ui, api, and mcp are separate packages within the repo. This would achieve some separation (you could even publish them separately if needed) without the overhead of multiple repositories. An incremental refactor would focus on **surgical improvements**: for example, unify the duplicate voice debug systems (pick one implementation and delete the other)[\[31\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L76-L84), remove the old OAuth code and rely purely on the current MCP proxy, strip out the legacy wallet routes in favor of the new ones[\[32\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L82-L90), and so on. Essentially, you’d be **finishing the half-done refactoring** and then improving from there.

During refactor, you’d also address the structural issues by, say, moving the MCP server out of the token-ai/ subdirectory into its own top-level folder (or its own repo eventually), and confining all web UI files to another folder. At runtime it could still be one project, but logically you’d treat them as separate. You might also convert more of the codebase to TypeScript to catch errors earlier. For example, many of the voice agent bugs were type/contract mismatches (like using wrong ID types or missing fields) – in TS, defining the shape of OpenAI events and tool schemas could have flagged those mismatches during compile or at least made the intended data flow clearer.

The upside of refactoring in place is you **preserve the working pieces more directly**. If certain things are currently stable (perhaps the basic chat without voice, or the wallet/trading logic after fixes), you won’t risk breaking them by rewriting from scratch. You can focus on one part at a time (e.g. get the voice agent stable using the SDK or switching to a simpler method, then move on to the next issue). It might also be easier to test changes incrementally. Given you already have systemd and deployment set up for the combined system, you could continue to use that while improving internals.

However, the downside is that the code will **continue to carry historical baggage**. It can be psychologically draining to work in a codebase that feels “dirty”, even as you clean it – the temptation to scrap it might grow if refactoring reveals more hidden problems. Also, some architectural leaps (like splitting into separate repos or adopting a new framework) are harder to do gradually – they might require temporarily broken states or big bang changes, which diminishes the advantage of incrementalism.

**Which to choose?** Considering the severity of the issues and your own sentiment (“just about to cry” with this code base), a **fundamental redo** sounds appropriate. The current state isn’t just a bit of clutter; it has systemic problems that could impede future development. A fresh start, informed by all you’ve learned, is likely to result in a cleaner, more maintainable system. That said, it’s important to **not lose the spirit** of what you’ve built – the new version should retain all the critical functionality. The good news is you have clarity on those functional requirements now, perhaps more than when you started the first version.

## Recommendations for a Revamped Architecture

Based on the above, here’s a plan of action to overhaul Dexter while preserving its core capabilities:

**1\. Separate the Frontend and Backend (Logical or Physical Split):** It will help immensely to partition the “website UI” from the “AI/agent service.” At minimum, create separate folders (or repos) for these. For example, all the static files and client-side code (HTML, CSS, JS) could live in a frontend directory or a new repo like dexter-ui. The MCP and related server code would go into dexter-mcp. This aligns with your instinct that the MCP should have been its own repo. By doing this, you achieve a clear contract: the front-end interacts with the MCP via HTTP (through /mcp endpoints) and doesn’t directly call internal functions. Likewise, the MCP doesn’t need to serve static assets or handle web-specific routing – it only cares about AI sessions. In practice, you can still deploy them together (e.g. same server via Nginx proxying, as you do now), but as a developer you’ll know which side of the fence a piece of code belongs to.

- _Frontend considerations:_ You might continue with the lightweight static approach (HTML + vanilla JS) for simplicity, or take the opportunity to use a modern framework (React, Svelte, etc.) to better manage state for the chat interface. A modern frontend could improve maintainability of the UI (for instance, React components for the chat messages, voice recorder, etc., instead of manipulating DOM directly). However, even a refined vanilla JS approach can work if it’s organized – maybe modularize the JS (public/js/ has files like auth.js, live/voice.js, etc.). In any case, the front-end should be **purely responsible for presentation and user interaction**, _not_ business logic. Right now, some logic is buried in the front-end (e.g. the voice client does some tool result handling in tools.js on the client side[\[33\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/REVIEW_OF_VOICE_AGENT_FIXES.md#L22-L26), and the UI mediates tool disambiguation like prompting the user to pick a token from results[\[34\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/REVIEW_OF_VOICE_AGENT_FIXES.md#L24-L26)). You’ll want to evaluate how much of that can be simplified. Perhaps the client shouldn’t have to handle tool outputs at all beyond displaying them – the agent could be designed to ask the user via the normal message flow instead of client-side prompts. Strive for a **thin client** that just sends user input and plays AI output (text or voice), while the heavy logic lives server-side.
- _Backend API considerations:_ If you spin off a small backend for non-AI functions (user profiles, linking accounts, etc.), design its API clearly and keep it separate from the MCP’s responsibilities. For example, account linking (if that means connecting a user’s wallet or external accounts) could be its own service or at least isolated route. This service would use Supabase/Postgres to store any user data as needed (profiles, linked wallet addresses, preferences, etc.). It can also handle security – verifying Supabase JWTs or session tokens before allowing access to MCP (you might already do this via the userToken mechanism in the /mcp-proxy path)[\[35\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L98-L101). Essentially, the web backend ensures that only authorized users can reach the MCP, and perhaps logs or meters usage per user.

**2\. Embrace Supabase Fully:** Since you indicated **“nothing local”** for the database, make completing the Supabase migration a priority in the redesign. This means using Supabase’s Postgres as the single source of truth for any persistent data: user accounts, saved chats or analysis reports, wallet info, etc. Moving fully to Supabase will let you drop any dev-only database configurations and a lot of conditional code. For example, you can remove the TOKEN_AI_DEMO_MODE paths and always enforce auth (in dev, you can still use Supabase’s anon key for a test user flow). Using Supabase Auth on the frontend (magic links or OAuth providers) means you don’t have to build a custom user system – just make sure your backend trusts and verifies the Supabase tokens. Supabase’s client libraries can simplify this.

In the MCP context, Supabase can also be used to store user-specific data needed by tools. One idea: instead of writing analysis artifacts to files in token-ai/reports, you could have the MCP log results to a database table (or Supabase Storage bucket) so they’re accessible later or across deployments. Similarly, trading or wallet data that the MCP tools use could reside in the DB (or fetched via APIs). The key is to avoid any local-only state; every piece of data that matters should be in the cloud DB so that the system is stateless between restarts (making it easier to maintain and scale). In short, **finish the transition to a cloud-based persistence layer** and eliminate leftover local file usage except perhaps for transient caches or logs.

**3\. Reimplement the Voice Agent with the Official SDK or Simplified Approach:** The voice feature is extremely important to you, so it needs to be rock-solid – but the current DIY implementation is fragile. You have two main options:

- **Use OpenAI’s Agents SDK (TypeScript)** to handle the real-time voice interaction. This SDK will manage the WebRTC/WebSocket connections, audio streaming, and even some tool integration for you[\[27\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L8-L15)[\[36\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L12-L19). By using it, you align with the architecture OpenAI expects, which means fewer low-level mistakes. For example, the SDK likely handles the call IDs and function outputs correctly, and it automatically chooses the optimal transport (WebRTC in browser)[\[36\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L12-L19). You can still integrate your custom tools – the SDK allows defining tools and will trigger your server when those functions are called[\[37\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L24-L29). Essentially, the Agents SDK could replace a lot of the custom code in agent-live.html and related client scripts with a cleaner interface. It might also offer better support for barge-in, silence detection, etc., or at least let you plug in OpenAI’s recommended approach to voice activity detection[\[38\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L31-L40)[\[39\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L48-L57). Adopting the SDK likely means writing some new TypeScript code (perhaps a small client library that your frontend uses, plus adapting the server to the SDK’s patterns), but this investment will pay off in reliability.
- **Or, adopt the “chained” voice architecture** (speech-to-text + text AI + text-to-speech) as a stopgap or simpler solution. The OpenAI docs mention a simpler approach where you transcribe audio to text, feed it to GPT-4, then synthesize voice from the text reply[\[40\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L44-L52). This is more traditional and might not offer the same low-latency, emotive interaction as the direct speech-to-speech model, but it’s easier to implement and debug. If the current S2S (speech-to-speech) setup continues to prove too flaky, you could fall back to this method temporarily: use Whisper (or the new gpt-4o-transcribe model) for user speech, use GPT-4 (text) with function calls for tools, then use a TTS service (like ElevenLabs or even the new GPT-4 voice for output) to speak. The user experience might be slightly less fluid than true streaming, but it could be acceptable, and importantly it would rely on well-understood components. That said, given your goal of a cutting-edge voice agent, you probably want to stick with the real-time approach – just do so with the proper support (i.e., the SDK, and thorough testing).

Regardless of approach, **simplify the roles** in the voice system: The browser/client should capture audio and play audio – not manage conversation state beyond that. The MCP (with help of OpenAI’s backend) should handle the conversation, including tool calls. Your fixes documentation shows the client was managing a lot: sending conversation.item.create messages, mapping tool outputs, etc.[\[41\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L11-L19)[\[42\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L25-L33). Ideally, much of that logic can move to the server (or be handled by the SDK’s client library). Aim for the client to just send audio stream and display text/voice outputs, while the server coordinates all function calls and state. This will reduce those 90% error scenarios since the single source of truth (the server/SDK) will be less likely to get out of sync.

**4\. Streamline Tool Usage – Rely on AI where possible, but keep critical tools local:** The **spirit** of your design is that the AI (GPT-4 or Claude) is extremely capable and should handle most reasoning and even some actions. You’re rightly skeptical of maintaining parallel “local intelligence.” However, completely removing local tools isn’t practical, because the AI ultimately needs some executor for actions (it can’t, for example, actually fetch a URL or execute a trade by itself without calling an API). What you can do is simplify and trust the AI more:

- Keep the **set of tools lean and essential.** It looks like you already went in this direction by scoping toolsets (you allow limiting to only reports,web or trading etc. as needed)[\[20\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L14-L22). You should continue with this philosophy in the rebuild. Don’t carry over any tool that isn’t pulling its weight. If you had plans for local analytical tools that the AI would call (beyond what’s needed for the core functionality), consider dropping them or postponing until the basics are solid. Focus on the **core tools** that deliver unique value: e.g. resolve_token, list_managed_wallets, execute_trade, maybe a browse_web (if you have a web search), and get_wallet_balance – whatever the AI truly needs to fulfill user requests. By reducing the number of tools, you reduce the surface for errors and the context size bloat[\[43\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L12-L20).
- Offload logic to the AI’s reasoning. For instance, instead of having complex client-side logic to disambiguate which token the user meant (reading out top 3 results and waiting for user choice)[\[33\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/REVIEW_OF_VOICE_AGENT_FIXES.md#L22-L26), you could let the AI handle that via dialogue: the assistant can ask the user **via text/voice** “Which of these tokens do you mean?” This keeps the conversation flow within the AI realm, rather than breaking it out into a custom UI prompt. The more you let the AI drive, the less custom code you need, as long as the AI is capable. GPT-4 is quite capable of multi-step reasoning when given proper context and tools (and now that your fixes have enabled multi-step tool chaining in one turn)[\[33\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/REVIEW_OF_VOICE_AGENT_FIXES.md#L22-L26). So trust it: your system prompt and few-shot examples can encourage the AI to handle ambiguous cases gracefully. Reserve client-side or server-side intervention only for absolute necessities (like a hard failure or a long pause detection).
- **Eliminate redundant local fallback logic.** If you have any code paths where a local function _attempts to do the same thing the AI tool would_, that can go. A hypothetical example: if there was a local code path to analyze text in case GPT failed – remove it and instead focus on why GPT failed and handle that (either by a retry or a graceful error message from the AI). In the new architecture, each tool will be a simple function that does one thing (query a DB, call an external API, perform a calculation) and returns the result to the model. The intelligence around using those results stays with the model. This way, your backend acts mostly as a **toolbox/API** and not as a decision-maker. This should align with your original intent of the MCP being the facilitator and the AI being the problem-solver.
- Continue to provide integration for external AI clients like ChatGPT and Claude **via standard protocols**. Your MCP already can interface with ChatGPT (through the OAuth and the expected “search”/“fetch” tools)[\[11\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L91-L100) and with Claude (likely via OAuth JWT as well)[\[44\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L61-L70). In a revamped setup, ensure this remains possible or even easier. By having the MCP as its own service with an OAuth2/OIDC layer, you essentially enable any OIDC-compatible client to connect. This is a strength – it means you don’t have to build separate implementations for each AI provider. Just make sure to document and test that interface. You might not need to run separate code for “Claude connector” vs “ChatGPT connector” – a unified OAuth-protected endpoint (as you have) can serve both, given the right configuration. Simpler code, fewer branches.

**5\. Improve Code Maintainability with Conventions and Types:** In the new version, enforce stricter **coding conventions** to prevent the code chaos from creeping back in. This includes:

- Using **TypeScript for all new code** (or as much as possible). Since Node 20 supports ESM, you can still run TS via a build step or use ts-node for dev. TypeScript will help catch errors like the wrong structure of an OpenAI message object or a missing field in a tool response. Your current mix of JS and TS might have been due to speed of development, but now it’s probably worth having the type safety, especially for complex interactions (the Agents SDK is in TS, which is another reason to lean that way). You already had a TS project for the inspector; extending that practice to the main agent code will increase reliability.
- Setting up **linting/formatting and tests** early. This may sound like secondary concern, but given the complexity, automated checks are your friend. A lint rule might have caught unused variables or the presence of that if(false) blocks lingering around. A proper test suite for the MCP tools (unit tests for each function, integration tests for an example agent conversation) will ensure that as you rebuild, you don’t reintroduce the old bugs. You can reuse some of the smoke tests you already had (like npm run test:mcp which likely simulates a basic conversation)[\[45\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L32-L36). Expand on those so that every tool call and scenario (voice agent asking to trade, etc.) is covered. This will give you confidence in the new architecture.
- **Modularize** aggressively. No single file should again approach 1700 lines – that’s a sign to split logic into modules. For example, break out the tool definitions, the voice session management, the OpenAI API interface, the database operations each into separate files or classes. You can have an organized folder structure where, say, mcp/tools/ has one file per tool category (trading, web, etc.), mcp/agents/ could hold the agent logic or prompts, and server/ (or a separate service) holds the web/API routes. Clear separation will prevent the kind of tangle seen in the current server.js. It also makes it feasible in the future to truly split into repos or packages by simply extracting those folders to new projects.
- **Remove all obsolete code** and configs. Don’t carry over anything that was a workaround for the old design (like PM2 vs systemd differences – irrelevant if you start anew with just systemd)[\[46\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L10-L18)[\[47\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L185-L193), or old environment variables that are no longer needed. This cleanup will ensure new contributors (or “future you”) don’t trip over misleading artifacts. It sounds obvious, but the current repo shows how easy it is for stale code to remain and cause confusion. So, be diligent in keeping the new codebase lean.

**6\. Preserve and build on the “good parts”:** It’s worth identifying what has _worked well_ in the current system so you make sure not to regress on those. For example:

- The **OAuth2/OIDC integration** for securing the MCP is a strong feature. It allows integration with real identity providers and ensures only authorized access. Keep this concept in the new design. It might be even simpler now – since you’re standardizing on Supabase for user auth, you could potentially use Supabase’s JWT in place of a separate OAuth layer (i.e., the front-end could pass its Supabase auth token to the MCP, which verifies it). Alternatively, continue with the dedicated OIDC flow if you plan to allow third-party AI clients (ChatGPT, etc.) to connect directly – in that case, your MCP remains an OAuth server as it is now. Either way, **do not drop authentication/authorization**; it’s needed for a multi-user system.
- The idea of **toolset scoping** to minimize context for the AI is good[\[43\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L12-L20). This improves reliability and cost. Carry that forward: initialize sessions with only the tools needed for that user’s task. In a rebuild, you could design the API such that when a user starts a conversation, the front-end indicates what they plan to do (or you infer it). For instance, if they open the “Trading Assistant” interface, you enable trading tools; if they’re just chatting or doing research, you might only enable web/search tools. This keeps prompts concise. You might also preserve the ability to configure it via env or API (as you have with TOKEN_AI_MCP_TOOLSETS). That flexibility is useful for debugging or special modes.
- The **multi-step tool chaining capability** you achieved after fixes is a valuable asset. It means the AI can autonomously perform a sequence of actions (e.g., identify a token, then get price, then execute a buy) in one continuous flow[\[33\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/REVIEW_OF_VOICE_AGENT_FIXES.md#L22-L26). In rebuilding, ensure your agent prompt and logic continue to allow that. You’ll want to port over the prompt engineering and domain instructions that made it possible. From your notes, the domain-specific system prompt for trading (which mentions the tools like list_managed_wallets) is important[\[48\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L49-L58). Don’t lose those carefully crafted prompts and instructions – they encapsulate business logic that would be time-consuming to rediscover. Instead, organize them (maybe in a prompts/ directory as you have) and load them into the new agent. The difference is now you’ll be confident the tools are correctly registered and the SDK will deliver outputs, so the model can actually follow those instructions reliably.
- The overall **vision of the platform** – a one-stop personal AI assistant that can talk, listen, and act (with the user’s accounts/crypto/etc.) – is absolutely still achievable. The new architecture is about making that vision maintainable. We are not removing any major feature; we’re re-implementing them in a cleaner way. So, keep the end-user perspective in mind: they should still be able to log in, start a voice chat or text chat with the AI, and have the AI perform tasks for them (like fetch info, manage a wallet, make a trade) seamlessly. Each of these user-facing features should be tested end-to-end in the new setup to ensure parity with (or improvements on) the old system.

**7\. Plan the transition carefully:** If you do opt for a full rebuild, consider how to transition without too much downtime or loss of data. Since you’re likely the only user at the moment (or a small user base), you can develop the new version in parallel. Use a separate database instance for it at first (or a new schema on Supabase) so you don’t mess with production data. Once it’s ready, you can swap the services and migrate any necessary data (for instance, you might migrate user accounts to Supabase if they were not already). Because Supabase is central, ensure you don’t lose things like existing wallet info or user settings – migrate those into the new DB if needed.

If refactoring in-place, plan feature toggles: you can refactor one part and test while the rest remains running. For example, you could switch the voice agent to the SDK while keeping everything else the same, verify it works, then proceed to split the repos, etc. Incremental refactor requires discipline to not break everything at once.

Taking a step back, **my recommendation is to lean towards a fresh rebuild with a modular design**, given how deep the issues run. The current repo served well as an experimental sandbox, but now it’s hindering progress. By rebuilding, you’ll free yourself from legacy constraints and likely reignite your enthusiasm as things “just work” more often. Crucially, you won’t be starting from zero – you have a wealth of knowledge (and code snippets) about what _not_ to do and what the system needs to do. Use that as a foundation for the new version.

To summarize the key changes in bullet form:

- **Split the Monolith:** Create separate projects (or at least modules) for the front-end UI, the general web backend, and the AI MCP server. This will improve focus and maintainability (each can be updated or replaced independently) and reflects the actual deployment architecture (separate UI and MCP services)[\[3\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L90-L98).
- **Use Supabase for All Auth & Data:** Finalize the migration by using Supabase’s database and auth everywhere. Remove fallback modes and local file writes. This simplification will reduce config overhead and ensure consistency between dev and prod[\[49\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L132-L140).
- **Rebuild Voice on Solid Ground:** Don’t continue fighting the custom voice integration if it’s 90% broken. Use OpenAI’s supported methods (the Realtime SDK, or a simpler pipeline) to get a stable voice agent. This will resolve the persistent “tool call errors” and timing issues by aligning with tested patterns[\[27\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L8-L15). The result should be a voice chat that _just works_ rather than one that requires constant patching.
- **Simplify and Trust the AI:** Let the AI model do the heavy lifting in conversation management. Provide it with only the essential tools implemented as reliable backend functions, and keep your custom code minimal and focused. Eliminate any duplicate logic or unused “tools” so the AI isn’t distracted and you aren’t maintaining code that the AI could replace with reasoning.
- **Clean Code Practices:** Adopt TypeScript, modularize the codebase, and enforce cleanliness (no leftover disabled blocks). By doing this, you prevent the new foundation from devolving into a tangle. For instance, the previous server file grew unwieldy and mixed concerns; the new code should break that apart so no single piece becomes a bottleneck or single point of failure in understanding.
- **Retain Core Functionality:** In the process, carry over the successful elements: robust OAuth security[\[44\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L61-L70), the domain-specific prompts/instructions, the improved multi-tool workflow, and the user-centric design (easy login, one-click to start chatting). The end product of the rebuild should feel to users like a more **reliable and responsive** version of the same application – all the features, but none of the flakiness.

Implementing these recommendations will take effort, but it addresses the root causes of the current pain. Instead of patching holes in a leaky ship, you’ll be building a new ship with watertight compartments. In the long run, this will save time and sanity. The fact that you have extensive documentation (agents, operations guides, fix notes)[\[14\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L54-L63)[\[48\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L49-L58) means you won’t lose knowledge during the rebuild – you have references to ensure no important detail is forgotten.

Finally, don’t be afraid that splitting into multiple repos or services will be “too much” overhead. It may feel that way initially, but given the scope of Dexter, separation is beneficial. You’ll thank yourself later when, say, the web frontend can be modified without risking the MCP code, or when you can restart the AI service without dropping the website. Even if you keep it in one repo for now, a clear separation of concerns in code is mandatory for manageability.

**In conclusion**, a well-planned re-architecture – modular, clean, and leveraging official tools (Supabase, OpenAI SDKs) – will allow you to achieve the same ambitious functionality without the current chaos. You won’t lose the spirit of Dexter; in fact, you’ll likely _enhance_ it, because a stable foundation will let you innovate faster on new features instead of fighting old bugs. As one of your internal docs put it, the disabled and duplicated code in the current server _“represents THREE different systems that were partially implemented then abandoned”_[\[14\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L54-L63) – by starting fresh, you can implement each system **fully and correctly** in its proper place, and abandon nothing. Good luck with the rebuild – it’s the hard path in the short term, but it leads to a far more maintainable and scalable Dexter in the long term!

**Sources:**

- Dexter Repository Guidelines and Structure[\[1\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/AGENTS.md#L5-L11)[\[2\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L30-L38)
- Operational README (deployment and config details)[\[3\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L90-L98)[\[9\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L123-L131)
- Server Code Analysis (duplicate routes and technical debt)[\[13\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L3-L11)[\[14\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L54-L63)
- OpenAI Documentation on Building Voice Agents (recommendation to use Agents SDK)[\[27\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L8-L15)
- Internal Fix Notes for Voice Agent (tool call ID and function schema issues)[\[22\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L5-L13)[\[23\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L23-L31)
- MCP README (toolset scoping and ChatGPT integration notes)[\[20\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L14-L22)[\[11\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L91-L100)

[\[1\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/AGENTS.md#L5-L11) AGENTS.md

<https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/AGENTS.md>

[\[2\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L30-L38) [\[3\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L90-L98) [\[7\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L84-L88) [\[8\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L122-L130) [\[9\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L123-L131) [\[19\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L76-L84) [\[28\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L124-L130) [\[46\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L10-L18) [\[47\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L185-L193) [\[49\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md#L132-L140) README.md

<https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/README.md>

[\[4\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/server.js#L64-L73) [\[5\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/server.js#L14-L21) server.js

<https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/server.js>

[\[6\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L1-L9) [\[10\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L75-L83) [\[11\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L91-L100) [\[12\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L97-L105) [\[20\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L14-L22) [\[21\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L102-L110) [\[35\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L98-L101) [\[43\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L12-L20) [\[44\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L61-L70) [\[45\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md#L32-L36) README.md

<https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/mcp/README.md>

[\[13\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L3-L11) [\[14\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L54-L63) [\[15\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L73-L81) [\[16\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L8-L17) [\[17\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L19-L28) [\[18\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L34-L42) [\[29\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L71-L79) [\[30\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L98-L105) [\[31\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L76-L84) [\[32\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md#L82-L90) SERVER_CLEANUP_ANALYSIS.md

<https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/docs/SERVER_CLEANUP_ANALYSIS.md>

[\[22\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L5-L13) [\[23\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L23-L31) [\[24\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L33-L37) [\[25\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L47-L56) [\[26\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L51-L58) [\[41\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L11-L19) [\[42\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L25-L33) [\[48\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md#L49-L58) FIX_VOICE_AGENT.md

<https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/FIX_VOICE_AGENT.md>

[\[27\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L8-L15) [\[36\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L12-L19) [\[37\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L24-L29) [\[38\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L31-L40) [\[39\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L48-L57) [\[40\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md#L44-L52) voice-agents.md

<https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/token-ai/docs/openai/realtime/voice-agents.md>

[\[33\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/REVIEW_OF_VOICE_AGENT_FIXES.md#L22-L26) [\[34\]](https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/REVIEW_OF_VOICE_AGENT_FIXES.md#L24-L26) REVIEW_OF_VOICE_AGENT_FIXES.md

<https://github.com/BranchManager69/dexter/blob/2ce574fee6c52443b535405ec786bd3b9538d1dc/notes/REVIEW_OF_VOICE_AGENT_FIXES.md>